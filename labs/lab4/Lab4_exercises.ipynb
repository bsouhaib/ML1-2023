{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2022-2023 - UMONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H3g2zh1W2t_c"
   },
   "source": [
    "**During the last lab, we learned how to fit a simple model to a single or multiple features to predict a given target variable. However, by fixing the number of features and by fixing the model's hyperparameters beforehand, we restricted ourselves to a single model. By doing so, we omitted to explore a broader range of models, one of which might better explain the relationship between our input and target variables.**\n",
    "\n",
    "**In this lab, we'll experiment with the general methodology of model selection, meaning that we'll define a set of predefined models, and we'll retain the one that minimizes the out-of-sample error.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1U5VE1l3YtKA"
   },
   "source": [
    "**Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zkt8eCSmYkbz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I_O6yz5QY8bH"
   },
   "source": [
    "**1) In this lab, we will work with the [Fish market](https://www.kaggle.com/datasets/aungpyaeap/fish-market) dataset, which contains several characteristics about fish, such as their weights, lengths, and species. Load the dataset as a Pandas dataframe, inspect its properties and check for any missing values. Change the data type of 'Species' to 'category'.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ARI4jGJDZ24R"
   },
   "source": [
    "**2) Does the dataframe contain any missing values ? If yes, replace the missing values by the sample mean for continuous variables. For missing categorical variables, replace them by the most frequent occurence of the corresponding column. You can use the `SimpleImputer` class of scikit-learn.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) As in the previous lab, create a one-hot-encoding of the categorical variable using the `OneHotEncoder` class.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) We will start by predicting the target 'Height' from the feature 'Weight'. Split your datasets into a training and test set following a 60/40 partition using the `train_test_split` function. Then generate a scatter plot of the two variables.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U_PEwHV0f14c"
   },
   "source": [
    "**5) We can see that a linear model would not be the best option to model the relationship between these variables. Instead of fitting a linear model, let's fit a polynomial model of specified degree.**\n",
    "\n",
    "- **Create your model using `PolynomialRegression` with `degree=2` and make sure to understand what the `make_pipeline` method does.**\n",
    "- **Fit your model with 10-fold cross-validation using the `cross_validate` function.**\n",
    "- **Report the mean of the test MSE across each folds based on the result of `cross_validate`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1a-6zXngJgZ",
    "outputId": "6da8ea7c-de9e-402b-8414-53c4e1062508"
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(2, fit_intercept=True)\n",
    "cv_results = cross_validate(model, X_train, y_train, cv=10, scoring=['neg_mean_squared_error'])\n",
    "\n",
    "test_mse = -cv_results['test_neg_mean_squared_error'].mean()\n",
    "print(f'Test MSE for a polynomial model of degree 2: {test_mse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aZm20YCJjb2j"
   },
   "source": [
    "**6) Let's now see how predictions vary with the model complexity. For polynomial degrees between 1 and 5, repeatedly fit the polynomial regression model (without cross-validation) and plot the predictions of 'Height' in function of the feature 'Weight'.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Plot the evolution of the train and test MSE of the 5 models from the previous question in function of the polynomial degree. What do you observe?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) In the rest of this lab, we will predict the target 'Height' based on all available features. Split again your dataset into a training and test set following a 60/40 partition using the `train_test_split` function.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Usually, models have more than one hyperparameter that can be tuned in order to find the model that best captures the relationship between our input and target variables. For instance, in the case of a simple linear regression using a polynomial transformation on the input variables, the hyperparameter space would be the polymial's degree, and whether or not to fit the intercept. Inspecting each combination of hyperparameters and selecting the combination that results in the best model is called grid search. However, manually inspecting each combination could be a very tedious task, but fortunatly, scikit-learn provides a class `GridSearchCV()` that implements this protocol for you.**\n",
    "\n",
    "**Select all features and 'Height' as target variable, and perform a grid search on the hyperparameter space of a polynomial regression model. Search for degrees varying between 1 and 5, and whether or not the intercept should be fit. Report the best hyperparameters and the corresponding MSE.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) Finally, fit a model using the best hyperparameters on the full training dataset and report the MSE.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
