{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2022-2023 - UMONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will experiment with the notorious 'Bias-Variance trade-off', a highly important concept in Machine Learning. In the context of a regression task, you've seen in the course that there exists an elegant decomposition of the MSE, which allows to illustrate to concept nicely. Contrary to previous labs in which we worked with real datasets, we will today generate our own dataset from a specified data generating process. This will enable us to estimate the bias and the variance of a given model.** \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Import the necessary libraries.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Create a function 'data_generating_process' that returns the following data generating process:**\n",
    "\n",
    "$$y = f(x) + \\epsilon,$$ \n",
    "\n",
    "**where $f(x) = 10\\text{sin}(x) + \\frac{x^3}{1000} - 2x$ and $\\epsilon \\sim \\mathcal{N}(0,10)$, i.e. $\\epsilon$ is random Gaussian noise with $\\mu=0$ and $\\sigma = 10$.**\n",
    "\n",
    "**The arguments of the function should be:**\n",
    "- **x : an array of values on which to evaluate $f(x)$**\n",
    "- **'add_noise' : a boolean. If True, the function should return $f(x) + \\epsilon$, if False, it should return $y=f(x)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Using the 'data_generating_process' function of the previous point, compute $f(x)$ and $y$ for the array $x$ below. Then, plot $f(x)$ and $y$ them on the same figure. Use a line plot for $f(x)$, and a scatter plot for $y$.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,50, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Fit a simple regression model to the data by following these steps:**\n",
    "- **Place the arrays $x$, $y$ and $f(x)$ in a Dataframe.** \n",
    "- **Select $x$ as the predictor, $y$ as the target variable.** \n",
    "- **Perform a train/test split with ratios 0.8/0.2.**\n",
    "- **Fit a simple linear regression model to the training set, and predict on the test set.**\n",
    "- **Report the test MSE.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Add the estimated regression line to the previously generated plot. You'll need to predict on the full array $x$. Does the model appear to be a good fit to the data ? How do you link this observation to the bias/variance trade-off ?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) We will now decompose our models' test MSE into a bias, variance and Bayes error term. Recall from the course that the MSE decomposition is given by:**\n",
    "\n",
    "$$\\mathbb{E}_{\\mathcal{D},y,x}[(y-g(x))^2)] = \\underbrace{\\text{Var}(y)}_{\\text{Bayes Error}} + \\underbrace{\\mathbb{E}_x[(f(x)-\\mathbb{E}_\\mathcal{D}[g(x)])^2]}_{\\text{Bias}} + \\underbrace{\\mathbb{E}_x[\\text{Var}(g(x))]}_{\\text{Variance}}$$\n",
    "\n",
    "**where $\\mathcal{D}$ is a training set sampled indenpendently from $p_{x,y}$. To evaluate this expression, we will:**\n",
    "- **For $j=1,...,J$, sample a dataset $\\mathcal{D}_j$ containing $n$ observations $(x_i,y_i)$ from the data generating process.**\n",
    "- **Split $\\mathcal{D}_j$ into a training set $\\mathcal{D}_{j,\\text{train}}$ and a test set $\\mathcal{D}_{j,\\text{test}}$ containing $n_{\\text{train}}$ and $n_{\\text{test}}$ observations respectively.**\n",
    "- **Fit the model to the training set, yielding $g_j$.**\n",
    "- **Make predictions on the test set using $g_j$.**\n",
    "\n",
    "**The above steps will allow us to obtain an estimate of the Bias Variance of the fitted models:**\n",
    "$$\\mathbb{E}_{\\mathcal{D},y,x}[(y-g(x))^2)] \\simeq \\underbrace{\\text{Var}(y)}_{\\text{Bayes Error}} + \\underbrace{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_\\text{test}} [(f(x_i) - \\frac{1}{J} \\sum_{j=1}^J g_j(x_i)^2)]}_{\\text{Bias}} + \\underbrace{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\text{Var}(g(x_i))}_{\\text{Variance}}.$$\n",
    "\n",
    "**To this end, complete the 'bias_variance_estimator' function below, which takes as arguments:**\n",
    "- **model : A scikit-learn model.**\n",
    "- **num_datasets : The number of datasets the sample from the data generating process.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_estimator(model, num_datasets):\n",
    "    predictions, targets, f_tests, errors = [], [], [], []\n",
    "    x_true = np.linspace(0,50, 100)\n",
    "    for _ in range(num_datasets):\n",
    "        y_true = data_generating_process(x_true)\n",
    "        df = pd.DataFrame(data={'x':x_true, 'y':y_true})\n",
    "        x = df[['x']]\n",
    "        y = df[['y']]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y, train_size=0.1, test_size=0.9, shuffle=True, random_state=0)\n",
    "        #Your code here\n",
    "        mse = mean_squared_error(y_pred_test, y_test)\n",
    "        predictions.append(y_pred_test)\n",
    "        targets.append(y_test)\n",
    "        f_tests.append(f_test)\n",
    "        errors.append(mse)\n",
    "    predictions = np.squeeze(np.array(predictions))\n",
    "    targets = np.squeeze(np.array(targets))\n",
    "    f_tests = np.squeeze(np.array(f_tests))\n",
    "    #Your code here\n",
    "    mse = np.mean(errors)\n",
    "    return bias, variance, bayes_error, mse\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Check that your implementation of the 'bias_variance_estimator' function is correct by comparing** \n",
    "$$\\text{Bayes Error} + \\text{Bias} + \\text{Variance}$$\n",
    "\n",
    "**to $\\frac{1}{n_\\text{test}} \\sum_{j=1}^J \\text{MSE}_j$, where $\\text{Bayes Error}$, $\\text{Bias}$, and $\\text{Variance}$ are returned by the 'bias_variance_estimator' function. To this end, employ a simple linear regression model, and sample 1000 datasets $\\mathcal{D}_j$ from the data generating process. Does the value obtained for the Bayes Error make sense ?**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) We will now study of the bias and variance of a model evolve when varying its complexity. To this end, apply the following steps:**\n",
    "- **Re-use the 'PolynomialRegression' function of Lab 4.**\n",
    "- **For polynomial degrees between 1 and 7, compute the bias, variance and Bayes error of the model and put them in a list.**\n",
    "- **On the same plot, show the evolution of the bias, variance and Bayes error terms as a function of the Polynomial degree.**\n",
    "\n",
    "**What are your observation ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Show the regression line for polynomial LinearRegression models of degrees in [1,7]. To this end, do:**\n",
    "- **From the Dataframe defined in 4), select $x$ as the predictor and $y$ as the target variable.**\n",
    "- **Split the dataset into training and test sets following a 01/0.9 ratio.**\n",
    "- **For each polynomial degree:**\n",
    "    - **Fit the model to the training set, and save it in a list 'models'.**\n",
    "- **For each model in models, predict on all $x$ and plot the predictions.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) Repeat the same exercise as in 8), but for the number of neighbors in a 'KNeighborsRegressor'.**\n",
    "\n",
    "**Are the evolution of the bias, variance and Bayes error term expected ?**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
